epochs: 3
eval_batch_size: 32
fp16: true
hira:
  alpha: 8
  dropout: 0.05
  l1_lambda: 0.0
  prune_ratio: 0.0
  r: 4
  target_modules:
  - q_lin
  - k_lin
  - v_lin
  - out_lin
lr: 1e-5
max_seq_len: 128
model_name: distilbert-base-uncased
output_dir: outputs/imdb_distilbert_hira
save_every_epochs: 1
seed: 685
task_name: imdb
train_batch_size: 16
warmup_ratio: 0.06
weight_decay: 0.02
