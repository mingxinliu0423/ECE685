epochs: 3
eval_batch_size: 32
fp16: true
lr: 2e-4
max_seq_len: 128
model_name: distilbert-base-uncased
output_dir: outputs/sst2_sparse_lora_prune
save_every_epochs: 3
seed: 685
sparse_lora:
  alpha: 16
  dropout: 0.05
  l1_lambda: 0.0
  method: prune
  prune_ratio: 0.3
  r: 8
  target_modules:
  - q_lin
  - k_lin
  - v_lin
  - out_lin
task_name: sst2
train_batch_size: 16
warmup_ratio: 0.06
weight_decay: 0.01
