epochs: 3
eval_batch_size: 32
fp16: true
lr: 2e-4
max_seq_len: 128
model_name: distilbert-base-uncased
output_dir: outputs/imdb_sparse_lora_l1
save_every_epochs: 3
seed: 685
sparse_lora:
  alpha: 16
  dropout: 0.05
  l1_lambda: 1e-3
  method: l1
  prune_ratio: 0.0
  r: 8
  target_modules:
  - q_lin
  - k_lin
  - v_lin
  - out_lin
sparse_warmup_ratio: 0.1
task_name: imdb
train_batch_size: 16
warmup_ratio: 0.06
weight_decay: 0.01
