seed: 685
model_name: distilbert-base-uncased
task_name: sst2
max_seq_len: 128
train_batch_size: 16
eval_batch_size: 32
epochs: 2
lr: 2e-4
weight_decay: 0.01
warmup_ratio: 0.06
fp16: true
output_dir: outputs/sst2_sparse_lora_l1_test
save_every_epochs: 1

# Sparse LoRA configuration with L1 regularization
sparse_lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: ["q_lin","k_lin","v_lin","out_lin"]
  method: "l1"           # Use L1 regularization
  l1_lambda: 1e-5        # L1 strength (test value)
  prune_ratio: 0.0       
