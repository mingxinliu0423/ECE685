seed: 685
model_name: distilbert-base-uncased
task_name: imdb
max_seq_len: 128
train_batch_size: 16
eval_batch_size: 32
epochs: 3
lr: 2e-4
weight_decay: 0.01
warmup_ratio: 0.06
fp16: true
output_dir: outputs/imdb_sparse_lora_svd

sparse_lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: ["q_lin", "k_lin", "v_lin", "out_lin"]

  method: "svd"

  svd_rank: 4

  ffn_sparsity: 0.75


  layer_sparsity:
    "layer.0": 0.4
    "layer.1": 0.5
    "layer.2": 0.6
    "layer.3": 0.8
    "layer.4": 0.9
    "layer.5": 0.95

sparse_warmup_ratio: 0.2

save_every_epochs: 3
